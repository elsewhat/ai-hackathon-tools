openapi: 3.0.0
info:
  description: Azure OpenAI APIs for completions and search
  title: Azure OpenAI Service API
  version: 2023-09-01-preview
servers:
- url: "https://{endpoint}/openai"
  variables:
    endpoint:
      default: your-resource-name.openai.azure.com
security:
- bearer:
  - api.read
- apiKey: []
paths:
  /deployments/{deployment-id}/completions:
    post:
      operationId: Completions_Create
      parameters:
      - explode: false
        in: path
        name: deployment-id
        required: true
        schema:
          description: Deployment id of the model which was deployed.
          example: davinci
          type: string
        style: simple
      - explode: true
        in: query
        name: api-version
        required: true
        schema:
          description: api version
          example: 2023-09-01-preview
          type: string
        style: form
      requestBody:
        content:
          application/json:
            example:
              prompt: |-
                Negate the following sentence.The price for bubblegum increased on thursday.

                 Negated Sentence:
              max_tokens: 50
            schema:
              $ref: '#/components/schemas/Completions_Create_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              example:
                model: davinci
                object: text_completion
                id: cmpl-4509KAos68kxOqpE2uYGw81j6m7uo
                created: 1637097562
                choices:
                - index: 0
                  text: The price for bubblegum decreased on thursday.
                  logprobs: null
                  finish_reason: stop
              schema:
                $ref: '#/components/schemas/Completions_Create_200_response'
          description: OK
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              explode: false
              schema:
                type: string
              style: simple
        default:
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/errorResponse'
          description: Service unavailable
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              explode: false
              schema:
                type: string
              style: simple
      summary: "Creates a completion for the provided prompt, parameters and chosen\
        \ model."
  /deployments/{deployment-id}/embeddings:
    post:
      operationId: embeddings_create
      parameters:
      - description: The deployment id of the model which was deployed.
        explode: false
        in: path
        name: deployment-id
        required: true
        schema:
          example: ada-search-index-v1
          type: string
        style: simple
      - explode: true
        in: query
        name: api-version
        required: true
        schema:
          description: api version
          example: 2023-09-01-preview
          type: string
        style: form
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/embeddings_create_request'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/embeddings_create_200_response'
          description: OK
      summary: Get a vector representation of a given input that can be easily consumed
        by machine learning models and algorithms.
  /deployments/{deployment-id}/chat/completions:
    post:
      operationId: ChatCompletions_Create
      parameters:
      - explode: false
        in: path
        name: deployment-id
        required: true
        schema:
          description: Deployment id of the model which was deployed.
          type: string
        style: simple
      - explode: true
        in: query
        name: api-version
        required: true
        schema:
          description: api version
          example: 2023-09-01-preview
          type: string
        style: form
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/createChatCompletionRequest'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/createChatCompletionResponse'
          description: OK
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              explode: false
              schema:
                type: string
              style: simple
        default:
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/errorResponse'
          description: Service unavailable
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              explode: false
              schema:
                type: string
              style: simple
      summary: Creates a completion for the chat message
  /deployments/{deployment-id}/extensions/chat/completions:
    post:
      operationId: ExtensionsChatCompletions_Create
      parameters:
      - explode: false
        in: path
        name: deployment-id
        required: true
        schema:
          description: Deployment id of the model which was deployed.
          type: string
        style: simple
      - explode: true
        in: query
        name: api-version
        required: true
        schema:
          description: api version
          example: 2023-09-01-preview
          type: string
        style: form
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/extensionsChatCompletionsRequest'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/extensionsChatCompletionsResponse'
          description: OK
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              explode: false
              schema:
                type: string
              style: simple
        default:
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/errorResponse'
          description: Service unavailable
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              explode: false
              schema:
                type: string
              style: simple
      summary: Using extensions to creates a completion for the chat messages.
  /deployments/{deployment-id}/audio/transcriptions:
    post:
      operationId: Transcriptions_Create
      parameters:
      - explode: false
        in: path
        name: deployment-id
        required: true
        schema:
          description: Deployment id of the whisper model.
          example: whisper
          type: string
        style: simple
      - description: Api version.
        explode: true
        in: query
        name: api-version
        required: true
        schema:
          description: Api version.
          example: 2023-05-20-preview
          type: string
        style: form
      requestBody:
        content:
          multipart/form-data:
            schema:
              $ref: '#/components/schemas/createTranscriptionRequest'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Transcriptions_Create_200_response'
            text/plain:
              schema:
                description: "Transcribed text in the output format (when response_format\
                  \ was one of text, vtt or srt)."
                type: string
          description: OK
      summary: Transcribes audio into the input language.
  /deployments/{deployment-id}/audio/translations:
    post:
      operationId: Translations_Create
      parameters:
      - explode: false
        in: path
        name: deployment-id
        required: true
        schema:
          description: Deployment id of the whisper model which was deployed.
          example: whisper
          type: string
        style: simple
      - description: Api version.
        explode: true
        in: query
        name: api-version
        required: true
        schema:
          description: Api version.
          example: 2023-05-20-preview
          type: string
        style: form
      requestBody:
        content:
          multipart/form-data:
            schema:
              $ref: '#/components/schemas/createTranslationRequest'
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Transcriptions_Create_200_response'
            text/plain:
              schema:
                description: "Transcribed text in the output format (when response_format\
                  \ was one of text, vtt or srt)."
                type: string
          description: OK
      summary: Transcribes and translates input audio into English text.
components:
  schemas:
    errorResponse:
      properties:
        error:
          $ref: '#/components/schemas/error'
      type: object
    errorBase:
      example:
        code: code
        message: message
      properties:
        code:
          type: string
        message:
          type: string
      type: object
    error:
      allOf:
      - $ref: '#/components/schemas/errorBase'
      properties:
        code:
          type: string
        message:
          type: string
        param:
          type: string
        type:
          type: string
        inner_error:
          $ref: '#/components/schemas/innerError'
      type: object
    innerError:
      description: Inner error with additional details.
      properties:
        code:
          $ref: '#/components/schemas/innerErrorCode'
        content_filter_results:
          $ref: '#/components/schemas/contentFilterResults'
      type: object
    innerErrorCode:
      description: Error codes for the inner error object.
      enum:
      - ResponsibleAIPolicyViolation
      type: string
      x-ms-enum:
        name: InnerErrorCode
        modelAsString: true
        values:
        - value: ResponsibleAIPolicyViolation
          description: The prompt violated one of more content filter rules.
    contentFilterResult:
      example:
        severity: safe
        filtered: true
      properties:
        severity:
          enum:
          - safe
          - low
          - medium
          - high
          type: string
          x-ms-enum:
            name: ContentFilterSeverity
            modelAsString: true
            values:
            - value: safe
              description: General content or related content in generic or non-harmful
                contexts.
            - value: low
              description: Harmful content at a low intensity and risk level.
            - value: medium
              description: Harmful content at a medium intensity and risk level.
            - value: high
              description: Harmful content at a high intensity and risk level.
        filtered:
          type: boolean
      required:
      - filtered
      - severity
      type: object
    contentFilterResults:
      description: "Information about the content filtering category (hate, sexual,\
        \ violence, self_harm), if it has been detected, as well as the severity level\
        \ (very_low, low, medium, high-scale that determines the intensity and risk\
        \ level of harmful content) and if it has been filtered or not."
      example:
        self_harm:
          severity: safe
          filtered: true
        hate:
          severity: safe
          filtered: true
        error:
          code: code
          message: message
        sexual:
          severity: safe
          filtered: true
        violence:
          severity: safe
          filtered: true
      properties:
        sexual:
          $ref: '#/components/schemas/contentFilterResult'
        violence:
          $ref: '#/components/schemas/contentFilterResult'
        hate:
          $ref: '#/components/schemas/contentFilterResult'
        self_harm:
          $ref: '#/components/schemas/contentFilterResult'
        error:
          $ref: '#/components/schemas/errorBase'
      type: object
    promptFilterResult:
      description: Content filtering results for a single prompt in the request.
      example:
        content_filter_results:
          self_harm:
            severity: safe
            filtered: true
          hate:
            severity: safe
            filtered: true
          error:
            code: code
            message: message
          sexual:
            severity: safe
            filtered: true
          violence:
            severity: safe
            filtered: true
        prompt_index: 6
      properties:
        prompt_index:
          type: integer
        content_filter_results:
          $ref: '#/components/schemas/contentFilterResults'
      type: object
    promptFilterResults:
      description: "Content filtering results for zero or more prompts in the request.\
        \ In a streaming request, results for different prompts may arrive at different\
        \ times or in different orders."
      items:
        $ref: '#/components/schemas/promptFilterResult'
      type: array
    chatCompletionsRequestCommon:
      properties:
        temperature:
          default: 1
          description: |-
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
            We generally recommend altering this or `top_p` but not both.
          example: 1
          maximum: 2
          minimum: 0
          nullable: true
          type: number
        top_p:
          default: 1
          description: |-
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
            We generally recommend altering this or `temperature` but not both.
          example: 1
          maximum: 1
          minimum: 0
          nullable: true
          type: number
        stream:
          default: false
          description: "If set, partial message deltas will be sent, like in ChatGPT.\
            \ Tokens will be sent as data-only server-sent events as they become available,\
            \ with the stream terminated by a `data: [DONE]` message."
          nullable: true
          type: boolean
        stop:
          $ref: '#/components/schemas/chatCompletionsRequestCommon_stop'
        max_tokens:
          default: 4096
          description: "The maximum number of tokens allowed for the generated answer.\
            \ By default, the number of tokens the model can return will be (4096\
            \ - prompt tokens)."
          type: integer
        presence_penalty:
          default: 0
          description: "Number between -2.0 and 2.0. Positive values penalize new\
            \ tokens based on whether they appear in the text so far, increasing the\
            \ model's likelihood to talk about new topics."
          maximum: 2
          minimum: -2
          type: number
        frequency_penalty:
          default: 0
          description: "Number between -2.0 and 2.0. Positive values penalize new\
            \ tokens based on their existing frequency in the text so far, decreasing\
            \ the model's likelihood to repeat the same line verbatim."
          maximum: 2
          minimum: -2
          type: number
        logit_bias:
          description: "Modify the likelihood of specified tokens appearing in the\
            \ completion. Accepts a json object that maps tokens (specified by their\
            \ token ID in the tokenizer) to an associated bias value from -100 to\
            \ 100. Mathematically, the bias is added to the logits generated by the\
            \ model prior to sampling. The exact effect will vary per model, but values\
            \ between -1 and 1 should decrease or increase likelihood of selection;\
            \ values like -100 or 100 should result in a ban or exclusive selection\
            \ of the relevant token."
          nullable: true
          type: object
        user:
          description: "A unique identifier representing your end-user, which can\
            \ help Azure OpenAI to monitor and detect abuse."
          example: user-1234
          nullable: false
          type: string
      type: object
    createChatCompletionRequest:
      allOf:
      - $ref: '#/components/schemas/chatCompletionsRequestCommon'
      - properties:
          messages:
            description: "A list of messages comprising the conversation so far. [Example\
              \ Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)."
            items:
              $ref: '#/components/schemas/chatCompletionRequestMessage'
            minItems: 1
            type: array
          functions:
            description: A list of functions the model may generate JSON inputs for.
            items:
              $ref: '#/components/schemas/chatCompletionFunctions'
            minItems: 1
            type: array
          function_call:
            $ref: '#/components/schemas/createChatCompletionRequest_allOf_function_call'
          "n":
            default: 1
            description: How many chat completion choices to generate for each input
              message.
            example: 1
            maximum: 128
            minimum: 1
            nullable: true
            type: integer
      required:
      - messages
      type: object
    chatCompletionFunctions:
      properties:
        name:
          description: "The name of the function to be called. Must be a-z, A-Z, 0-9,\
            \ or contain underscores and dashes, with a maximum length of 64."
          type: string
        description:
          description: The description of what the function does.
          type: string
        parameters:
          additionalProperties: true
          description: "The parameters the functions accepts, described as a JSON\
            \ Schema object. See the [guide](/docs/guides/gpt/function-calling) for\
            \ examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/)\
            \ for documentation about the format."
          type: object
      required:
      - name
      type: object
    chatCompletionFunctionParameters:
      additionalProperties: true
      description: "The parameters the functions accepts, described as a JSON Schema\
        \ object. See the [guide](/docs/guides/gpt/function-calling) for examples,\
        \ and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/)\
        \ for documentation about the format."
      type: object
    chatCompletionRequestMessage:
      properties:
        role:
          description: "The role of the messages author. One of `system`, `user`,\
            \ `assistant`, or `function`."
          enum:
          - system
          - user
          - assistant
          - function
          type: string
        content:
          description: The contents of the message. `content` is required for all
            messages except assistant messages with function calls.
          type: string
        name:
          description: "The name of the author of this message. `name` is required\
            \ if role is `function`, and it should be the name of the function whose\
            \ response is in the `content`. May contain a-z, A-Z, 0-9, and underscores,\
            \ with a maximum length of 64 characters."
          type: string
        function_call:
          $ref: '#/components/schemas/chatCompletionRequestMessage_function_call'
      required:
      - role
      type: object
    createChatCompletionResponse:
      allOf:
      - $ref: '#/components/schemas/chatCompletionsResponseCommon'
      - properties:
          prompt_filter_results:
            $ref: '#/components/schemas/promptFilterResults'
          choices:
            items:
              $ref: '#/components/schemas/createChatCompletionResponse_allOf_choices_inner'
            type: array
      required:
      - choices
      - created
      - id
      - model
      - object
      type: object
    chatCompletionResponseMessage:
      properties:
        role:
          description: The role of the author of this message.
          enum:
          - system
          - user
          - assistant
          - function
          type: string
        content:
          description: The contents of the message.
          type: string
        function_call:
          $ref: '#/components/schemas/chatCompletionRequestMessage_function_call'
      required:
      - role
      type: object
    extensionsChatCompletionsRequest:
      allOf:
      - $ref: '#/components/schemas/chatCompletionsRequestCommon'
      - properties:
          messages:
            items:
              $ref: '#/components/schemas/message'
            type: array
          dataSources:
            description: The data sources to be used for the Azure OpenAI on your
              data feature.
            items:
              $ref: '#/components/schemas/dataSource'
            type: array
      description: Request for the chat completions using extensions
      example:
        dataSources:
        - type: AzureCognitiveSearch
          parameters:
            endpoint: https://mysearchexample.search.windows.net
            key: '***(admin key)'
            indexName: my-chunk-index
            fieldsMapping:
              titleField: productName
              urlField: productUrl
              filepathField: productFilePath
              contentFields:
              - productDescription
              contentFieldsSeparator: |2+

            topNDocuments: 5
            queryType: semantic
            semanticConfiguration: defaultConfiguration
            inScope: true
            roleInformation: roleInformation
        messages:
        - role: user
          content: Where can I find a hiking place in Seattle?
        temperature: 0.9
      required:
      - messages
      type: object
    dataSource:
      description: The data source to be used for the Azure OpenAI on your data feature.
      properties:
        type:
          description: The data source type.
          type: string
        parameters:
          additionalProperties: true
          description: The parameters to be used for the data source in runtime.
          type: object
      required:
      - type
      type: object
    message:
      description: A chat message.
      properties:
        index:
          description: The index of the message in the conversation.
          type: integer
        role:
          description: The role of the author of this message.
          enum:
          - system
          - user
          - assistant
          - tool
          type: string
        recipient:
          description: The recipient of the message in the format of <namespace>.<operation>.
            Present if and only if the recipient is tool.
          example: Contoso.productsUsingGET
          type: string
        content:
          description: The contents of the message
          type: string
        end_turn:
          description: Whether the message ends the turn.
          type: boolean
        context:
          $ref: '#/components/schemas/message_context'
      required:
      - content
      - role
      type: object
    chatCompletionsResponseCommon:
      properties:
        id:
          type: string
        object:
          type: string
        created:
          format: unixtime
          type: integer
        model:
          type: string
        usage:
          $ref: '#/components/schemas/chatCompletionsResponseCommon_usage'
      required:
      - created
      - id
      - model
      - object
      type: object
    chatCompletionChoiceCommon:
      properties:
        index:
          type: integer
        finish_reason:
          type: string
      type: object
    extensionsChatCompletionChoice:
      allOf:
      - $ref: '#/components/schemas/chatCompletionChoiceCommon'
      - properties:
          message:
            $ref: '#/components/schemas/message'
      type: object
    extensionsChatCompletionsResponse:
      allOf:
      - $ref: '#/components/schemas/chatCompletionsResponseCommon'
      - properties:
          choices:
            items:
              $ref: '#/components/schemas/extensionsChatCompletionChoice'
            type: array
      description: The response of the extensions chat completions.
      example:
        id: "1"
        object: extensions.chat.completion
        created: 1679201802
        model: gpt-3.5-turbo-0301
        choices:
        - index: 0
          finish_reason: stop
          message:
            role: assistant
            content: "Seattle is a great place for hiking! Here are some of the best\
              \ hiking places in Seattle according to Contoso Traveler [doc1] and\
              \ West Coast Traveler, Snow Lake, Mount Si, and Mount Tenerife [doc2].\
              \ I hope this helps! Let me know if you need more information."
            end_turn: true
            context:
              messages:
              - role: tool
                content: "{\"citations\":[{\"filepath\":\"ContosoTraveler.pdf\",\"\
                  content\":\"This is the content of the citation 1\"},{\"filepath\"\
                  :\"WestCoastTraveler.html\",\"content\":\"This is the content of\
                  \ the citation 2\"},{\"content\":\"This is the content of the citation\
                  \ 3 without filepath\"}],\"intent\":\"hiking place in seattle\"}"
                end_turn: false
      type: object
    createTranslationRequest:
      description: Translation request.
      properties:
        file:
          description: The audio file to translate.
          format: binary
          type: string
        prompt:
          description: An optional text to guide the model's style or continue a previous
            audio segment. The prompt should be in English.
          type: string
        response_format:
          $ref: '#/components/schemas/responseFormat'
        temperature:
          default: 0
          description: "The sampling temperature, between 0 and 1. Higher values like\
            \ 0.8 will make the output more random, while lower values like 0.2 will\
            \ make it more focused and deterministic. If set to 0, the model will\
            \ use log probability to automatically increase the temperature until\
            \ certain thresholds are hit."
          type: number
      required:
      - file
      type: object
    audioResponse:
      description: Translation or transcription response when response_format was
        json
      properties:
        text:
          description: Translated or transcribed text.
          type: string
      required:
      - text
      type: object
    audioVerboseResponse:
      allOf:
      - $ref: '#/components/schemas/audioResponse'
      - properties:
          task:
            description: Type of audio task.
            enum:
            - transcribe
            - translate
            type: string
            x-ms-enum:
              modelAsString: true
          language:
            description: Language.
            type: string
          duration:
            description: Duration.
            type: number
          segments:
            items:
              $ref: '#/components/schemas/audioSegment'
            type: array
      description: Translation or transcription response when response_format was
        verbose_json
      required:
      - text
      type: object
    responseFormat:
      description: Defines the format of the output.
      enum:
      - json
      - text
      - srt
      - verbose_json
      - vtt
      title: ResponseFormat
      type: string
      x-ms-enum:
        modelAsString: true
    createTranscriptionRequest:
      description: Transcription request.
      properties:
        file:
          description: The audio file object to transcribe.
          format: binary
          type: string
        prompt:
          description: An optional text to guide the model's style or continue a previous
            audio segment. The prompt should match the audio language.
          type: string
        response_format:
          $ref: '#/components/schemas/responseFormat'
        temperature:
          default: 0
          description: "The sampling temperature, between 0 and 1. Higher values like\
            \ 0.8 will make the output more random, while lower values like 0.2 will\
            \ make it more focused and deterministic. If set to 0, the model will\
            \ use log probability to automatically increase the temperature until\
            \ certain thresholds are hit."
          type: number
        language:
          description: The language of the input audio. Supplying the input language
            in ISO-639-1 format will improve accuracy and latency.
          type: string
      required:
      - file
      type: object
    audioSegment:
      description: Transcription or translation segment.
      properties:
        id:
          description: Segment identifier.
          type: integer
        seek:
          description: Offset of the segment.
          type: number
        start:
          description: Segment start offset.
          type: number
        end:
          description: Segment end offset.
          type: number
        text:
          description: Segment text.
          type: string
        tokens:
          description: Tokens of the text.
          items:
            nullable: false
            type: number
          type: array
        temperature:
          description: Temperature.
          type: number
        avg_logprob:
          description: Average log probability.
          type: number
        compression_ratio:
          description: Compression ratio.
          type: number
        no_speech_prob:
          description: Probability of 'no speech'.
          type: number
      type: object
    Completions_Create_request_prompt:
      description: |-
        The prompt(s) to generate completions for, encoded as a string or array of strings.
        Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. Maximum allowed size of string list is 2048.
      oneOf:
      - default: ""
        example: This is a test.
        nullable: true
        type: string
      - description: Array size minimum of 1 and maximum of 2048
        items:
          default: ""
          example: This is a test.
          nullable: false
          type: string
        type: array
    Completions_Create_request_stop:
      description: Up to 4 sequences where the API will stop generating further tokens.
        The returned text will not contain the stop sequence.
      oneOf:
      - default: <|endoftext|>
        example: |2+

        nullable: true
        type: string
      - description: Array minimum size of 1 and maximum of 4
        items:
          example: |2+

          nullable: false
          type: string
        type: array
    Completions_Create_request:
      properties:
        prompt:
          $ref: '#/components/schemas/Completions_Create_request_prompt'
        max_tokens:
          default: 16
          description: "The token count of your prompt plus max_tokens cannot exceed\
            \ the model's context length. Most models have a context length of 2048\
            \ tokens (except for the newest models, which support 4096). Has minimum\
            \ of 0."
          example: 16
          nullable: true
          type: integer
        temperature:
          default: 1
          description: |-
            What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
            We generally recommend altering this or top_p but not both.
          example: 1
          nullable: true
          type: number
        top_p:
          default: 1
          description: |-
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
            We generally recommend altering this or temperature but not both.
          example: 1
          nullable: true
          type: number
        logit_bias:
          description: "Defaults to null. Modify the likelihood of specified tokens\
            \ appearing in the completion. Accepts a json object that maps tokens\
            \ (specified by their token ID in the GPT tokenizer) to an associated\
            \ bias value from -100 to 100. You can use this tokenizer tool (which\
            \ works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically,\
            \ the bias is added to the logits generated by the model prior to sampling.\
            \ The exact effect will vary per model, but values between -1 and 1 should\
            \ decrease or increase likelihood of selection; values like -100 or 100\
            \ should result in a ban or exclusive selection of the relevant token.\
            \ As an example, you can pass {\"50256\" &#58; -100} to prevent the <|endoftext|>\
            \ token from being generated."
          nullable: false
          type: object
        user:
          description: "A unique identifier representing your end-user, which can\
            \ help monitoring and detecting abuse"
          nullable: false
          type: string
        "n":
          default: 1
          description: |-
            How many completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.
            Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.
          example: 1
          nullable: true
          type: integer
        stream:
          default: false
          description: "Whether to stream back partial progress. If set, tokens will\
            \ be sent as data-only server-sent events as they become available, with\
            \ the stream terminated by a data: [DONE] message."
          nullable: true
          type: boolean
        logprobs:
          description: |-
            Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
            Minimum of 0 and maximum of 5 allowed.
          nullable: true
          type: integer
        suffix:
          description: The suffix that comes after a completion of inserted text.
          nullable: true
          type: string
        echo:
          default: false
          description: Echo back the prompt in addition to the completion
          nullable: true
          type: boolean
        stop:
          $ref: '#/components/schemas/Completions_Create_request_stop'
        completion_config:
          nullable: true
          type: string
        presence_penalty:
          default: 0
          description: "Number between -2.0 and 2.0. Positive values penalize new\
            \ tokens based on whether they appear in the text so far, increasing the\
            \ model's likelihood to talk about new topics."
          type: number
        frequency_penalty:
          default: 0
          description: "Number between -2.0 and 2.0. Positive values penalize new\
            \ tokens based on their existing frequency in the text so far, decreasing\
            \ the model's likelihood to repeat the same line verbatim."
          type: number
        best_of:
          description: |-
            Generates best_of completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.
            When used with n, best_of controls the number of candidate completions and n specifies how many to return - best_of must be greater than n.
            Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. Has maximum value of 128.
          type: integer
      type: object
    Completions_Create_200_response_choices_inner_logprobs:
      example:
        top_logprobs:
        - key: 5.637376656633329
        - key: 5.637376656633329
        token_logprobs:
        - 5.962133916683182
        - 5.962133916683182
        tokens:
        - tokens
        - tokens
        text_offset:
        - 2
        - 2
      nullable: true
      properties:
        tokens:
          items:
            type: string
          type: array
        token_logprobs:
          items:
            type: number
          type: array
        top_logprobs:
          items:
            additionalProperties:
              type: number
            type: object
          type: array
        text_offset:
          items:
            type: integer
          type: array
      type: object
    Completions_Create_200_response_choices_inner:
      example:
        content_filter_results:
          self_harm:
            severity: safe
            filtered: true
          hate:
            severity: safe
            filtered: true
          error:
            code: code
            message: message
          sexual:
            severity: safe
            filtered: true
          violence:
            severity: safe
            filtered: true
        finish_reason: finish_reason
        index: 1
        text: text
        logprobs:
          top_logprobs:
          - key: 5.637376656633329
          - key: 5.637376656633329
          token_logprobs:
          - 5.962133916683182
          - 5.962133916683182
          tokens:
          - tokens
          - tokens
          text_offset:
          - 2
          - 2
      properties:
        text:
          type: string
        index:
          type: integer
        logprobs:
          $ref: '#/components/schemas/Completions_Create_200_response_choices_inner_logprobs'
        finish_reason:
          type: string
        content_filter_results:
          $ref: '#/components/schemas/contentFilterResults'
      type: object
    Completions_Create_200_response_usage:
      example:
        completion_tokens: 7.061401241503109
        prompt_tokens: 9.301444243932576
        total_tokens: 3.616076749251911
      properties:
        completion_tokens:
          format: int32
          type: number
        prompt_tokens:
          format: int32
          type: number
        total_tokens:
          format: int32
          type: number
      required:
      - completion_tokens
      - prompt_tokens
      - total_tokens
      type: object
    Completions_Create_200_response:
      example:
        created: 0
        prompt_filter_results:
        - content_filter_results:
            self_harm:
              severity: safe
              filtered: true
            hate:
              severity: safe
              filtered: true
            error:
              code: code
              message: message
            sexual:
              severity: safe
              filtered: true
            violence:
              severity: safe
              filtered: true
          prompt_index: 6
        - content_filter_results:
            self_harm:
              severity: safe
              filtered: true
            hate:
              severity: safe
              filtered: true
            error:
              code: code
              message: message
            sexual:
              severity: safe
              filtered: true
            violence:
              severity: safe
              filtered: true
          prompt_index: 6
        usage:
          completion_tokens: 7.061401241503109
          prompt_tokens: 9.301444243932576
          total_tokens: 3.616076749251911
        model: model
        id: id
        choices:
        - content_filter_results:
            self_harm:
              severity: safe
              filtered: true
            hate:
              severity: safe
              filtered: true
            error:
              code: code
              message: message
            sexual:
              severity: safe
              filtered: true
            violence:
              severity: safe
              filtered: true
          finish_reason: finish_reason
          index: 1
          text: text
          logprobs:
            top_logprobs:
            - key: 5.637376656633329
            - key: 5.637376656633329
            token_logprobs:
            - 5.962133916683182
            - 5.962133916683182
            tokens:
            - tokens
            - tokens
            text_offset:
            - 2
            - 2
        - content_filter_results:
            self_harm:
              severity: safe
              filtered: true
            hate:
              severity: safe
              filtered: true
            error:
              code: code
              message: message
            sexual:
              severity: safe
              filtered: true
            violence:
              severity: safe
              filtered: true
          finish_reason: finish_reason
          index: 1
          text: text
          logprobs:
            top_logprobs:
            - key: 5.637376656633329
            - key: 5.637376656633329
            token_logprobs:
            - 5.962133916683182
            - 5.962133916683182
            tokens:
            - tokens
            - tokens
            text_offset:
            - 2
            - 2
        object: object
      properties:
        id:
          type: string
        object:
          type: string
        created:
          type: integer
        model:
          type: string
        prompt_filter_results:
          description: "Content filtering results for zero or more prompts in the\
            \ request. In a streaming request, results for different prompts may arrive\
            \ at different times or in different orders."
          items:
            $ref: '#/components/schemas/promptFilterResult'
          type: array
        choices:
          items:
            $ref: '#/components/schemas/Completions_Create_200_response_choices_inner'
          type: array
        usage:
          $ref: '#/components/schemas/Completions_Create_200_response_usage'
      required:
      - choices
      - created
      - id
      - model
      - object
      type: object
    embeddings_create_request_input:
      description: |-
        Input text to get embeddings for, encoded as a string. To get embeddings for multiple inputs in a single request, pass an array of strings. Each input must not exceed 2048 tokens in length.
        Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single space, as we have observed inferior results when newlines are present.
      oneOf:
      - default: ""
        example: This is a test.
        nullable: true
        type: string
      - items:
          example: This is a test.
          minLength: 1
          nullable: false
          type: string
        maxItems: 2048
        minItems: 1
        type: array
    embeddings_create_request:
      additionalProperties: true
      properties:
        input:
          $ref: '#/components/schemas/embeddings_create_request_input'
        user:
          description: "A unique identifier representing your end-user, which can\
            \ help monitoring and detecting abuse."
          nullable: false
          type: string
        input_type:
          description: input type of embedding search to use
          example: query
          type: string
      required:
      - input
      type: object
    embeddings_create_200_response_data_inner:
      example:
        index: 0
        embedding:
        - 6.027456183070403
        - 6.027456183070403
        object: object
      properties:
        index:
          type: integer
        object:
          type: string
        embedding:
          items:
            type: number
          type: array
      required:
      - embedding
      - index
      - object
      type: object
    embeddings_create_200_response_usage:
      example:
        prompt_tokens: 1
        total_tokens: 5
      properties:
        prompt_tokens:
          type: integer
        total_tokens:
          type: integer
      required:
      - prompt_tokens
      - total_tokens
      type: object
    embeddings_create_200_response:
      example:
        data:
        - index: 0
          embedding:
          - 6.027456183070403
          - 6.027456183070403
          object: object
        - index: 0
          embedding:
          - 6.027456183070403
          - 6.027456183070403
          object: object
        usage:
          prompt_tokens: 1
          total_tokens: 5
        model: model
        object: object
      properties:
        object:
          type: string
        model:
          type: string
        data:
          items:
            $ref: '#/components/schemas/embeddings_create_200_response_data_inner'
          type: array
        usage:
          $ref: '#/components/schemas/embeddings_create_200_response_usage'
      required:
      - data
      - model
      - object
      - usage
      type: object
    Transcriptions_Create_200_response:
      oneOf:
      - $ref: '#/components/schemas/audioResponse'
      - $ref: '#/components/schemas/audioVerboseResponse'
    chatCompletionsRequestCommon_stop:
      default: null
      description: Up to 4 sequences where the API will stop generating further tokens.
      oneOf:
      - nullable: true
        type: string
      - description: Array minimum size of 1 and maximum of 4
        items:
          nullable: false
          type: string
        maxItems: 4
        minItems: 1
        type: array
    createChatCompletionRequest_allOf_function_call_oneOf:
      properties:
        name:
          description: The name of the function to call.
          type: string
      required:
      - name
      type: object
    createChatCompletionRequest_allOf_function_call:
      description: "Controls how the model responds to function calls. \"none\" means\
        \ the model does not call a function, and responds to the end-user. \"auto\"\
        \ means the model can pick between an end-user or calling a function.  Specifying\
        \ a particular function via `{\"name\":\\ \"my_function\"}` forces the model\
        \ to call that function. \"none\" is the default when no functions are present.\
        \ \"auto\" is the default if functions are present."
      oneOf:
      - enum:
        - none
        - auto
        type: string
      - $ref: '#/components/schemas/createChatCompletionRequest_allOf_function_call_oneOf'
    chatCompletionRequestMessage_function_call:
      description: "The name and arguments of a function that should be called, as\
        \ generated by the model."
      properties:
        name:
          description: The name of the function to call.
          type: string
        arguments:
          description: "The arguments to call the function with, as generated by the\
            \ model in JSON format. Note that the model does not always generate valid\
            \ JSON, and may hallucinate parameters not defined by your function schema.\
            \ Validate the arguments in your code before calling your function."
          type: string
      type: object
    createChatCompletionResponse_allOf_choices_inner:
      allOf:
      - $ref: '#/components/schemas/chatCompletionChoiceCommon'
      - properties:
          message:
            $ref: '#/components/schemas/chatCompletionResponseMessage'
          content_filter_results:
            $ref: '#/components/schemas/contentFilterResults'
      type: object
    message_context:
      description: The conversation context
      nullable: true
      properties:
        messages:
          description: Messages exchanged between model and extensions prior to final
            message from model
          items:
            $ref: '#/components/schemas/message'
          minItems: 1
          nullable: true
          type: array
      type: object
    chatCompletionsResponseCommon_usage:
      properties:
        prompt_tokens:
          type: integer
        completion_tokens:
          type: integer
        total_tokens:
          type: integer
      required:
      - completion_tokens
      - prompt_tokens
      - total_tokens
      type: object
  securitySchemes:
    bearer:
      flows:
        implicit:
          authorizationUrl: https://login.microsoftonline.com/common/oauth2/v2.0/authorize
          scopes: {}
      type: oauth2
      x-tokenInfoFunc: api.middleware.auth.bearer_auth
      x-scopeValidateFunc: api.middleware.auth.validate_scopes
    apiKey:
      in: header
      name: api-key
      type: apiKey

