/*
 * Azure OpenAI Service API
 *
 * Azure OpenAI APIs for completions and search
 *
 * The version of the OpenAPI document: 2023-09-01-preview
 * Generated by: https://github.com/openapitools/openapi-generator.git
 */


using System;
using System.Collections;
using System.Collections.Generic;
using System.Collections.ObjectModel;
using System.Linq;
using System.IO;
using System.Runtime.Serialization;
using System.Text;
using System.Text.RegularExpressions;
using Newtonsoft.Json;
using Newtonsoft.Json.Converters;
using Newtonsoft.Json.Linq;
using System.ComponentModel.DataAnnotations;
using OpenAPIDateConverter = AzureOpenAI.Client.OpenAPIDateConverter;

namespace AzureOpenAI.Model
{
    /// <summary>
    /// ChatCompletionsRequestCommon
    /// </summary>
    [DataContract(Name = "chatCompletionsRequestCommon")]
    public partial class ChatCompletionsRequestCommon : IEquatable<ChatCompletionsRequestCommon>, IValidatableObject
    {
        /// <summary>
        /// Initializes a new instance of the <see cref="ChatCompletionsRequestCommon" /> class.
        /// </summary>
        /// <param name="temperature">What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or &#x60;top_p&#x60; but not both. (default to 1M).</param>
        /// <param name="topP">An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or &#x60;temperature&#x60; but not both. (default to 1M).</param>
        /// <param name="stream">If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. (default to false).</param>
        /// <param name="stop">stop.</param>
        /// <param name="maxTokens">The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). (default to 4096).</param>
        /// <param name="presencePenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics. (default to 0M).</param>
        /// <param name="frequencyPenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim. (default to 0M).</param>
        /// <param name="logitBias">Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token..</param>
        /// <param name="user">A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse..</param>
        public ChatCompletionsRequestCommon(decimal? temperature = 1M, decimal? topP = 1M, bool? stream = false, ChatCompletionsRequestCommonStop stop = default(ChatCompletionsRequestCommonStop), int maxTokens = 4096, decimal presencePenalty = 0M, decimal frequencyPenalty = 0M, Object logitBias = default(Object), string user = default(string))
        {
            // use default value if no "temperature" provided
            this.Temperature = temperature ?? 1M;
            // use default value if no "topP" provided
            this.TopP = topP ?? 1M;
            // use default value if no "stream" provided
            this.Stream = stream ?? false;
            this.Stop = stop;
            this.MaxTokens = maxTokens;
            this.PresencePenalty = presencePenalty;
            this.FrequencyPenalty = frequencyPenalty;
            this.LogitBias = logitBias;
            this.User = user;
        }

        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or &#x60;top_p&#x60; but not both.
        /// </summary>
        /// <value>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or &#x60;top_p&#x60; but not both.</value>
        /// <example>1</example>
        [DataMember(Name = "temperature", EmitDefaultValue = true)]
        public decimal? Temperature { get; set; }

        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or &#x60;temperature&#x60; but not both.
        /// </summary>
        /// <value>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or &#x60;temperature&#x60; but not both.</value>
        /// <example>1</example>
        [DataMember(Name = "top_p", EmitDefaultValue = true)]
        public decimal? TopP { get; set; }

        /// <summary>
        /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message.
        /// </summary>
        /// <value>If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message.</value>
        [DataMember(Name = "stream", EmitDefaultValue = true)]
        public bool? Stream { get; set; }

        /// <summary>
        /// Gets or Sets Stop
        /// </summary>
        [DataMember(Name = "stop", EmitDefaultValue = true)]
        public ChatCompletionsRequestCommonStop Stop { get; set; }

        /// <summary>
        /// The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).
        /// </summary>
        /// <value>The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).</value>
        [DataMember(Name = "max_tokens", EmitDefaultValue = true)]
        public int MaxTokens { get; set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.</value>
        [DataMember(Name = "presence_penalty", EmitDefaultValue = true)]
        public decimal PresencePenalty { get; set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.</value>
        [DataMember(Name = "frequency_penalty", EmitDefaultValue = true)]
        public decimal FrequencyPenalty { get; set; }

        /// <summary>
        /// Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        /// </summary>
        /// <value>Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.</value>
        [DataMember(Name = "logit_bias", EmitDefaultValue = true)]
        public Object LogitBias { get; set; }

        /// <summary>
        /// A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.
        /// </summary>
        /// <value>A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.</value>
        /// <example>user-1234</example>
        [DataMember(Name = "user", EmitDefaultValue = true)]
        public string User { get; set; }

        /// <summary>
        /// Returns the string presentation of the object
        /// </summary>
        /// <returns>String presentation of the object</returns>
        public override string ToString()
        {
            StringBuilder sb = new StringBuilder();
            sb.Append("class ChatCompletionsRequestCommon {\n");
            sb.Append("  Temperature: ").Append(Temperature).Append("\n");
            sb.Append("  TopP: ").Append(TopP).Append("\n");
            sb.Append("  Stream: ").Append(Stream).Append("\n");
            sb.Append("  Stop: ").Append(Stop).Append("\n");
            sb.Append("  MaxTokens: ").Append(MaxTokens).Append("\n");
            sb.Append("  PresencePenalty: ").Append(PresencePenalty).Append("\n");
            sb.Append("  FrequencyPenalty: ").Append(FrequencyPenalty).Append("\n");
            sb.Append("  LogitBias: ").Append(LogitBias).Append("\n");
            sb.Append("  User: ").Append(User).Append("\n");
            sb.Append("}\n");
            return sb.ToString();
        }

        /// <summary>
        /// Returns the JSON string presentation of the object
        /// </summary>
        /// <returns>JSON string presentation of the object</returns>
        public virtual string ToJson()
        {
            return Newtonsoft.Json.JsonConvert.SerializeObject(this, Newtonsoft.Json.Formatting.Indented);
        }

        /// <summary>
        /// Returns true if objects are equal
        /// </summary>
        /// <param name="input">Object to be compared</param>
        /// <returns>Boolean</returns>
        public override bool Equals(object input)
        {
            return this.Equals(input as ChatCompletionsRequestCommon);
        }

        /// <summary>
        /// Returns true if ChatCompletionsRequestCommon instances are equal
        /// </summary>
        /// <param name="input">Instance of ChatCompletionsRequestCommon to be compared</param>
        /// <returns>Boolean</returns>
        public bool Equals(ChatCompletionsRequestCommon input)
        {
            if (input == null)
            {
                return false;
            }
            return 
                (
                    this.Temperature == input.Temperature ||
                    (this.Temperature != null &&
                    this.Temperature.Equals(input.Temperature))
                ) && 
                (
                    this.TopP == input.TopP ||
                    (this.TopP != null &&
                    this.TopP.Equals(input.TopP))
                ) && 
                (
                    this.Stream == input.Stream ||
                    (this.Stream != null &&
                    this.Stream.Equals(input.Stream))
                ) && 
                (
                    this.Stop == input.Stop ||
                    (this.Stop != null &&
                    this.Stop.Equals(input.Stop))
                ) && 
                (
                    this.MaxTokens == input.MaxTokens ||
                    this.MaxTokens.Equals(input.MaxTokens)
                ) && 
                (
                    this.PresencePenalty == input.PresencePenalty ||
                    this.PresencePenalty.Equals(input.PresencePenalty)
                ) && 
                (
                    this.FrequencyPenalty == input.FrequencyPenalty ||
                    this.FrequencyPenalty.Equals(input.FrequencyPenalty)
                ) && 
                (
                    this.LogitBias == input.LogitBias ||
                    (this.LogitBias != null &&
                    this.LogitBias.Equals(input.LogitBias))
                ) && 
                (
                    this.User == input.User ||
                    (this.User != null &&
                    this.User.Equals(input.User))
                );
        }

        /// <summary>
        /// Gets the hash code
        /// </summary>
        /// <returns>Hash code</returns>
        public override int GetHashCode()
        {
            unchecked // Overflow is fine, just wrap
            {
                int hashCode = 41;
                if (this.Temperature != null)
                {
                    hashCode = (hashCode * 59) + this.Temperature.GetHashCode();
                }
                if (this.TopP != null)
                {
                    hashCode = (hashCode * 59) + this.TopP.GetHashCode();
                }
                if (this.Stream != null)
                {
                    hashCode = (hashCode * 59) + this.Stream.GetHashCode();
                }
                if (this.Stop != null)
                {
                    hashCode = (hashCode * 59) + this.Stop.GetHashCode();
                }
                hashCode = (hashCode * 59) + this.MaxTokens.GetHashCode();
                hashCode = (hashCode * 59) + this.PresencePenalty.GetHashCode();
                hashCode = (hashCode * 59) + this.FrequencyPenalty.GetHashCode();
                if (this.LogitBias != null)
                {
                    hashCode = (hashCode * 59) + this.LogitBias.GetHashCode();
                }
                if (this.User != null)
                {
                    hashCode = (hashCode * 59) + this.User.GetHashCode();
                }
                return hashCode;
            }
        }

        /// <summary>
        /// To validate all properties of the instance
        /// </summary>
        /// <param name="validationContext">Validation context</param>
        /// <returns>Validation Result</returns>
        IEnumerable<System.ComponentModel.DataAnnotations.ValidationResult> IValidatableObject.Validate(ValidationContext validationContext)
        {
            // Temperature (decimal?) maximum
            if (this.Temperature > (decimal?)2)
            {
                yield return new System.ComponentModel.DataAnnotations.ValidationResult("Invalid value for Temperature, must be a value less than or equal to 2.", new [] { "Temperature" });
            }

            // Temperature (decimal?) minimum
            if (this.Temperature < (decimal?)0)
            {
                yield return new System.ComponentModel.DataAnnotations.ValidationResult("Invalid value for Temperature, must be a value greater than or equal to 0.", new [] { "Temperature" });
            }

            // TopP (decimal?) maximum
            if (this.TopP > (decimal?)1)
            {
                yield return new System.ComponentModel.DataAnnotations.ValidationResult("Invalid value for TopP, must be a value less than or equal to 1.", new [] { "TopP" });
            }

            // TopP (decimal?) minimum
            if (this.TopP < (decimal?)0)
            {
                yield return new System.ComponentModel.DataAnnotations.ValidationResult("Invalid value for TopP, must be a value greater than or equal to 0.", new [] { "TopP" });
            }

            // PresencePenalty (decimal) maximum
            if (this.PresencePenalty > (decimal)2)
            {
                yield return new System.ComponentModel.DataAnnotations.ValidationResult("Invalid value for PresencePenalty, must be a value less than or equal to 2.", new [] { "PresencePenalty" });
            }

            // PresencePenalty (decimal) minimum
            if (this.PresencePenalty < (decimal)-2)
            {
                yield return new System.ComponentModel.DataAnnotations.ValidationResult("Invalid value for PresencePenalty, must be a value greater than or equal to -2.", new [] { "PresencePenalty" });
            }

            // FrequencyPenalty (decimal) maximum
            if (this.FrequencyPenalty > (decimal)2)
            {
                yield return new System.ComponentModel.DataAnnotations.ValidationResult("Invalid value for FrequencyPenalty, must be a value less than or equal to 2.", new [] { "FrequencyPenalty" });
            }

            // FrequencyPenalty (decimal) minimum
            if (this.FrequencyPenalty < (decimal)-2)
            {
                yield return new System.ComponentModel.DataAnnotations.ValidationResult("Invalid value for FrequencyPenalty, must be a value greater than or equal to -2.", new [] { "FrequencyPenalty" });
            }

            yield break;
        }
    }

}
